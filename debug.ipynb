{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torchcam.methods import LayerCAM, SmoothGradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "import clip\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "from models.resnet import CustomResNet\n",
    "from models.projector import ProjectionHead, VisualTransformer\n",
    "from domainnet_data import DomainNetDataset, get_domainnet_loaders, get_data_from_saved_files\n",
    "from utils import SimpleDINOLoss, compute_accuracy, compute_similarities, plot_grad_flow, plot_confusion_matrix\n",
    "from prompts.FLM import generate_label_mapping_by_frequency, label_mapping_base\n",
    "\n",
    "\n",
    "to_pil = ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    \"\"\"\n",
    "    Load an image and convert it to a NumPy array with values in the range [0, 255].\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image as a NumPy array with values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    # Open the image file\n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    # Convert to RGB mode if not already in RGB\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Convert image to NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Ensure values are in the range [0, 255]\n",
    "    image_array = np.clip(image_array, 0, 255)\n",
    "\n",
    "    return image_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unnormalize(tensor):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    mean_tensor = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n",
    "    std_tensor = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n",
    "    tensor.mul_(std_tensor[:, None, None]).add_(mean_tensor[:, None, None])\n",
    "    return tensor\n",
    "\n",
    "def save_image(tensor, file_name):\n",
    "\n",
    "    tensor = tensor.detach().cpu()\n",
    "    # Ensure it's in the range [0, 1]\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "    # Convert to image and save\n",
    "    vutils.save_image(tensor, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.Resize((224, 224)),\n",
    "#     # transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                         std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "CLIP_custom_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "Resnet_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def gaussian_noise(x, severity=1):\n",
    "    c = [0., .08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
    "\n",
    "\n",
    "# PIL_image = CLIP_custom_transform(Image.open('./data/domainnet_v1.0/real/toothpaste/real_318_000284.jpg'))\n",
    "\n",
    "im = load_image('./data/domainnet_v1.0/real/toothpaste/real_318_000284.jpg')\n",
    "PIL_image = CLIP_custom_transform(Image.fromarray(gaussian_noise(im, severity=1).astype(np.uint8)))\n",
    "\n",
    "# l = torch.from_numpy(np.array([317]))\n",
    "# valset = torch.utils.data.TensorDataset(image, l)\n",
    "# val_loader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f\"logs/classifier/resnet50_domainnet_real\"\n",
    "data_dir = f\"data/domainnet_v1.0\"\n",
    "prompt_embeddings_pth = \"prompts/CLIP_RN50_text_embeddings.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "clip_model, preprocess = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(preprocess)\n",
    "clip_model.eval()\n",
    "\n",
    "# Load class names from a text file\n",
    "with open(os.path.join(data_dir, 'class_names.txt'), 'r') as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "loaders, _ = get_domainnet_loaders(\"real\", batch_size=10, data_dir=data_dir)\n",
    "\n",
    "train_loader = loaders['train']\n",
    "val_loader = loaders['test']\n",
    "\n",
    "text_encodings = torch.load(prompt_embeddings_pth)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image for clip\n",
    "image_CLIP = preprocess(PIL_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode the image using CLIP encoder_image\n",
    "clip_image_features = clip_model.encode_image(image_CLIP)\n",
    "\n",
    "# Compute similarities between image embeddings and text encodings\n",
    "orig_similarities_ = compute_similarities(clip_image_features, text_encodings, mode=\"cosine\")\n",
    "orig_prob_ = F.softmax(orig_similarities_, dim=-1)\n",
    "orig_predictions_ = torch.argmax(orig_prob_, dim=-1)\n",
    "\n",
    "\n",
    "print(f\"Original Zero-shot prediction: {class_names[orig_predictions_[0].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.hist(orig_prob_[0].detach().cpu().numpy(), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your trained model from checkpoint\n",
    "checkpoint = torch.load('logs/classifier/resnet50_domainnet_real/best_checkpoint.pth')\n",
    "\n",
    "resnet_model = CustomResNet(model_name='resnet50', num_classes=345, use_pretrained=True)\n",
    "resnet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "resnet_model.eval()\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "resnet_model.to('cuda')\n",
    "\n",
    "projector = ProjectionHead(input_dim=2048, output_dim=1024).to('cuda')\n",
    "projector.load_state_dict(torch.load('logs/classifier/resnet50_domainnet_real/projection_default_prompt_feat_sim0.1_distill1_DN_mapping1_scaled_logits/best_projector_weights.pth'))\n",
    "projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet_images = Resnet_transform(PIL_image).unsqueeze(0).to('cuda')\n",
    "# Get the ResNet predictions\n",
    "resnet_logits, resnet_embeddings = resnet_model(resnet_images, return_features=True)\n",
    "probs_from_resnet = F.softmax(resnet_logits, dim=-1)\n",
    "resnet_predictions = torch.argmax(probs_from_resnet, dim=-1)\n",
    "\n",
    "# Project the resnet embeddings\n",
    "proj_embeddings = projector(resnet_embeddings)\n",
    "# Compute the predictions using the projected embeddings\n",
    "similarities = compute_similarities(proj_embeddings, text_encodings, mode=\"DN\")\n",
    "probs_from_proj = F.softmax(similarities, dim=-1)\n",
    "proj_predictions = torch.argmax(probs_from_proj, dim=-1)\n",
    "\n",
    "print(f\"ResNet predictions: {class_names[resnet_predictions[0].item()]}\")\n",
    "print(f\"Projected predictions: {class_names[proj_predictions[0].item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(probs_from_resnet[0].detach().cpu().numpy(), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(probs_from_proj[0].detach().cpu().numpy(), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_image_features_norm = F.normalize(clip_image_features[0], dim=-1)\n",
    "proj_embeddings_norm = F.normalize(proj_embeddings[0], dim=-1)\n",
    "\n",
    "clip_text_features_norm = F.normalize(text_encodings[300], dim=-1)\n",
    "\n",
    "# plt.plot(clip_image_features_norm.detach().cpu().numpy(), label=\"CLIP\")\n",
    "plt.plot(proj_embeddings_norm.detach().cpu().numpy(), label=\"Projected\")\n",
    "plt.plot(clip_text_features_norm.detach().cpu().numpy(), label=\"Text\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DomainNetDataset(root_dir='data/domainnet_v1.0', domain='real', split='train', transform=None)\n",
    "\n",
    "\n",
    "all_clip_embeddings = []\n",
    "all_custom_clip_embeddings = []\n",
    "all_resnet_embeddings = []\n",
    "all_proj_embeddings = []\n",
    "for i in range(len(dataset)):\n",
    "    images, label = dataset[i]\n",
    "\n",
    "    resnet_images = Resnet_transform(images).unsqueeze(0).to('cuda')\n",
    "    # Get the ResNet predictions\n",
    "    resnet_logits, resnet_embeddings = resnet_model(resnet_images, return_features=True)\n",
    "\n",
    "    # Project the resnet embeddings\n",
    "    proj_embeddings = projector(resnet_embeddings)\n",
    "    all_proj_embeddings.append(proj_embeddings.detach().cpu())\n",
    "    \n",
    "    # Preprocess the image for clip\n",
    "    CLIP_images = preprocess(images).unsqueeze(0).to(device)\n",
    "    clip_image_features = clip_model.encode_image(CLIP_images)\n",
    "    all_clip_embeddings.append(clip_image_features.detach().cpu())\n",
    "\n",
    "    custom_CLIP_images = preprocess(CLIP_custom_transform(images)).unsqueeze(0).to(device)\n",
    "    custom_clip_image_features = clip_model.encode_image(custom_CLIP_images)\n",
    "    all_custom_clip_embeddings.append(custom_clip_image_features.detach().cpu())\n",
    "\n",
    "\n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "all_clip_embeddings = torch.cat(all_clip_embeddings, dim=0)\n",
    "all_proj_embeddings = torch.cat(all_proj_embeddings, dim=0)\n",
    "all_custom_clip_embeddings = torch.cat(all_custom_clip_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_umap_embeddings\n",
    "\n",
    "plot_umap_embeddings(all_clip_embeddings, all_proj_embeddings, text_encodings.detach().cpu(), labels=['CLIP image', 'Projected image', 'CLIP Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "CLIP_model, preprocess = clip.load(\"RN50\", device='cuda')\n",
    "dataset = CIFAR100(root=\"./data\", download=True, transform=None, train=False)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = CustomResNet(model_name='resnet50', num_classes=100, use_pretrained=True).to('cuda')\n",
    "\n",
    "projector_CIFAR100 = ProjectionHead(input_dim=2048, output_dim=1024).to('cuda')\n",
    "projector_imagenet = ProjectionHead(input_dim=2048, output_dim=1024).to('cuda')\n",
    "\n",
    "# Load projector weights from checkpoint\n",
    "projector_CIFAR100.load_state_dict(torch.load('logs/classifier/resnet50_cifar100/projection_default_prompt_DN_mapping1_scaled_logits/best_projector_weights.pth'))\n",
    "projector_CIFAR100.eval()\n",
    "projector_imagenet.load_state_dict(torch.load('logs/classifier/imagenet/contrastive_loss/resnet50_imagenet_None/projection_default_prompt_feat_sim0_distill1_cosine_mapping1_scaled_logits/best_projector_weights.pth'))\n",
    "projector_imagenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_cifar_100_projections = []\n",
    "all_imagenet_projections = []\n",
    "all_clip_embeddings = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    images, label = dataset[i]\n",
    "\n",
    "    resnet_images = transform(images).unsqueeze(0).to('cuda')\n",
    "    # Get the ResNet predictions\n",
    "    resnet_logits, resnet_embeddings = resnet_model(resnet_images, return_features=True)\n",
    "\n",
    "    # Project the resnet embeddings\n",
    "    proj_embeddings = projector_CIFAR100(resnet_embeddings)\n",
    "    all_cifar_100_projections.append(proj_embeddings.detach().cpu())\n",
    "\n",
    "    proj_embeddings = projector_imagenet(resnet_embeddings)\n",
    "    all_imagenet_projections.append(proj_embeddings.detach().cpu())\n",
    "    \n",
    "    # Preprocess the image for clip\n",
    "    CLIP_images = preprocess(images).unsqueeze(0).to(device)\n",
    "    clip_image_features = clip_model.encode_image(CLIP_images)\n",
    "    all_clip_embeddings.append(clip_image_features.detach().cpu())\n",
    "\n",
    "    if i == 300:\n",
    "        break\n",
    "\n",
    "all_cifar_100_projections = torch.cat(all_cifar_100_projections, dim=0).detach().cpu()\n",
    "all_imagenet_projections = torch.cat(all_imagenet_projections, dim=0).detach().cpu()\n",
    "all_clip_embeddings = torch.cat(all_clip_embeddings, dim=0).detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_umap_embeddings\n",
    "\n",
    "plot_umap_embeddings(all_cifar_100_projections, all_imagenet_projections, all_clip_embeddings, labels=['CIFAR100 Projection', 'Imagenet Projection', 'CLIP Image Encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import torch\n",
    "import cv2\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"checkpoints/sam_vit_h_4b8939.pth\").to('cuda')\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "sam_transform = predictor.transform\n",
    "sam_vit = sam.image_encoder\n",
    "\n",
    "test_image = sam_transform(torch.randn(5, 3, 224, 224)).to('cuda')\n",
    "\n",
    "print(sam.image_encoder.img_size)\n",
    "print(sam_vit(test_image).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./data/domainnet_v1.0/real/toothpaste/real_318_000284.jpg')\n",
    "predictor.set_image(image)\n",
    "image_embeddings = predictor.get_image_embedding()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "\n",
    "\n",
    "print(predictor.model.image_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "\n",
    "\n",
    "# To assist you in designing the feature extractor you may want to print out\n",
    "# the available nodes for resnet50.\n",
    "m = resnet50()\n",
    "train_nodes, eval_nodes = get_graph_node_names(resnet50())\n",
    "\n",
    "print(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
