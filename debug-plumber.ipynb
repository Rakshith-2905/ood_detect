{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# from torchcam.methods import LayerCAM, SmoothGradCAMpp\n",
    "# from torchcam.utils import overlay_mask\n",
    "\n",
    "import clip\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "from models.resnet import CustomResNet\n",
    "from models.projector import ProjectionHead\n",
    "from domainnet_data import DomainNetDataset, get_domainnet_loaders, get_data_from_saved_files\n",
    "from utils_proj import SimpleDINOLoss, compute_accuracy, compute_similarities, plot_grad_flow, plot_confusion_matrix\n",
    "from prompts.FLM import generate_label_mapping_by_frequency, label_mapping_base\n",
    "from models.resnet import CustomClassifier, CustomResNet\n",
    "import umap\n",
    "\n",
    "to_pil = ToPILImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_name, domain_name,train_transforms, test_transforms, clip_transform, data_dir='../data'):\n",
    "\n",
    "    if data_name == 'imagenet':\n",
    "        train_dataset = dset.ImageFolder(root=f'{data_dir}/imagenet_train_examples', transform=train_transforms)\n",
    "        val_dataset = dset.ImageFolder(root=f'{data_dir}/imagenet_val_examples', transform=test_transforms)\n",
    "        class_names = train_dataset.classes\n",
    "\n",
    "    elif data_name == 'domainnet':\n",
    "        train_dataset = DomainNetDataset(root_dir=data_dir, domain=domain_name, \\\n",
    "                                        split='train', transform=train_transforms, transform2=clip_transform)\n",
    "        val_dataset = DomainNetDataset(root_dir=data_dir, domain=domain_name, \\\n",
    "                                        split='test', transform=test_transforms, transform2=clip_transform)\n",
    "        class_names = train_dataset.class_names\n",
    "\n",
    "    return train_dataset, val_dataset, class_names\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embeddings(val_loader,classifier,clip_model,clip_text_encodings,projector,device):\n",
    "    all_clip_embeddings = []\n",
    "\n",
    "    all_classifier_embeddings = []\n",
    "    all_proj_embeddings = []\n",
    "    all_clip_text_embeddings = []\n",
    "    clip_text_encodings=clip_text_encodings.to(device)\n",
    "\n",
    "    for i,(images_batch, labels, images_clip_batch) in enumerate(val_loader):\n",
    "        images_batch = images_batch.to(device)\n",
    "        images_clip_batch = images_clip_batch.to(device)    \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        classifier_logits, classifier_embeddings = classifier(images_batch, return_features=True) # (batch_size, embedding_dim)\n",
    "\n",
    "        clip_image_embeddings = clip_model.encode_image(images_clip_batch) # (batch_size, embedding_dim)\n",
    "        \n",
    "        clip_image_embeddings = clip_image_embeddings.type_as(classifier_embeddings)\n",
    "\n",
    "        if PROJ_CLIP: # this is PLUMBER\n",
    "            proj_embeddings = projector(clip_image_embeddings) # (batch_size, projection_dim)\n",
    "        else: # this is LIMBER\n",
    "            proj_embeddings = projector(classifier_embeddings) # (batch_size, projection_dim)\n",
    "\n",
    "        all_clip_text_embeddings.append(clip_text_encodings[labels])\n",
    "        all_clip_embeddings.append(clip_image_embeddings.detach().cpu())\n",
    "        all_proj_embeddings.append(proj_embeddings.detach().cpu())\n",
    "        all_classifier_embeddings.append(classifier_embeddings.detach().cpu())\n",
    "        if i == 200:\n",
    "            break\n",
    "\n",
    "\n",
    "    all_clip_embeddings = torch.cat(all_clip_embeddings, dim=0)\n",
    "    all_proj_embeddings = torch.cat(all_proj_embeddings, dim=0)\n",
    "    all_classifier_embeddings = torch.cat(all_classifier_embeddings, dim=0)\n",
    "    all_clip_text_embeddings = torch.cat(all_clip_text_embeddings, dim=0)\n",
    "    return all_clip_embeddings, all_proj_embeddings, all_classifier_embeddings,all_clip_text_embeddings\n",
    "\n",
    "def build_classifier(classifier_name, num_classes, pretrained=False, checkpoint_path=None):\n",
    "\n",
    "    if classifier_name in ['vit_b_16', 'swin_b']:\n",
    "        classifier = CustomClassifier(classifier_name, use_pretrained=pretrained)\n",
    "    elif classifier_name in ['resnet18', 'resnet50']:\n",
    "        classifier = CustomResNet(classifier_name, num_classes=num_classes, use_pretrained=pretrained)\n",
    "\n",
    "    if checkpoint_path:\n",
    "        classifier.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n",
    "\n",
    "    train_transform = classifier.train_transform\n",
    "    test_transform = classifier.test_transform\n",
    "\n",
    "    return classifier, train_transform, test_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "data_dir = f\"/usr/workspace/KDML/DomainNet\"\n",
    "prompt_embeddings_pth = \"/usr/workspace/KDML/DomainNet/CLIP_ViT-B-32_text_encodings.pt\"\n",
    "classifier_name= \"resnet50\"\n",
    "num_classes = 345\n",
    "\n",
    "projector_weights_path= '/usr/workspace/KDML/ood_detect/checkpoints/painting_test_projector/best_projector_weights.pth'\n",
    "#projector_weights_path = \"/usr/workspace/KDML/ood_detect/resnet50_domainnet_real/plumber/resnet50domain_{sketch}_lr_0.1_is_mlp_False/projector_weights_final.pth\"\n",
    "checkpoint_path = f\"{data_dir}/best_checkpoint.pth\"\n",
    "PROJ_CLIP = True\n",
    "dataset_name=\"domainnet\"\n",
    "domain_name=\"clipart\"\n",
    "domainnet_domains_projector= {\"real\":'logs/classifier/domainnet/plumber/resnet50domain_real_lr_0.1_is_mlp_False/best_projector_weights.pth',\\\n",
    "                              \"sketch\": \"logs/classifier/domainnet/plumber/resnet50domain_sketch_lr_0.1_is_mlp_False/best_projector_weights.pth\",\\\n",
    "                             \"painting\": \"logs/classifier/domainnet/plumber/resnet50domain_painting_lr_0.1_is_mlp_False/best_projector_weights.pth\",\\\n",
    "                             \"clipart\": \"logs/classifier/domainnet/plumber/resnet50domain_clipart_lr_0.1_is_mlp_False/best_projector_weights.pth\"\n",
    "}      \n",
    "# Load class names from a text file\n",
    "with open(os.path.join(data_dir, 'class_names.txt'), 'r') as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
